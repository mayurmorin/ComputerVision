{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import functools as ft\n",
    "import itertools as it\n",
    "import operator as op\n",
    "import collections as co\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import os.path as osp\n",
    "\n",
    "\n",
    "from utilities import my_show, my_gshow, my_read, my_read_g, my_read_cg\n",
    "\n",
    "from sklearn import (cluster, datasets, decomposition,\n",
    "                     metrics,\n",
    "                     model_selection as skms,\n",
    "                     neighbors, pipeline, svm,\n",
    "                     preprocessing as skpre)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cm_helper(ax, actual, predicted):\n",
    "    cm = metrics.confusion_matrix(actual, predicted)\n",
    "    sns.heatmap(cm, annot=True, fmt='3d', ax=ax)\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    \n",
    "# reproducibilty is next to godliness\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,4)\n",
    "first_four_wtgt = it.islice(zip(digits.images, digits.target), 4)\n",
    "for (image, label), ax in zip(first_four_wtgt, axes):       \n",
    "    my_gshow(ax, image)\n",
    "    ax.set_title(\"True: {}\".format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Simplest Workflow with a Super-simple Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  1. create\n",
    "  2. fit\n",
    "  3. predict\n",
    "  4. lather-rinse-repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloading data so we have a one-cell example:\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# simplify some names and massage shape of data\n",
    "n_examples = len(digits.images)\n",
    "data = digits.images.reshape(n_examples, -1)  # N rows, rest is flattened\n",
    "tgts = digits.target\n",
    "\n",
    "# split the data into train/test sets\n",
    "(data_train, data_tst,\n",
    " tgts_train, tgts_tst) = skms.train_test_split(data, tgts, test_size=.2)\n",
    "\n",
    "# create and fit model\n",
    "knn_classifier = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifier.fit(data_train, tgts_train)\n",
    "\n",
    "# predict and evaluate result for first test case\n",
    "predicted = knn_classifier.predict(data_tst[0:1,:])[0] #annoying, wants 2D\n",
    "actual    = tgts_tst[0]\n",
    "\n",
    "my_gshow(plt.gca(), data_tst[0].reshape(8,8))\n",
    "plt.gca().set_title(\"actual: {}\\npredicted: {}\".format(actual, predicted));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for the entire test set and see how we did:\n",
    "predicted = knn_classifier.predict(data_tst)\n",
    "cm_helper(plt.gca(), tgts_tst, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some LOO Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Revisiting Last Week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incidentally, last week, we built a 1-NN classifer that used HOG features.  And we evaluated it using leave-one-out cross-validation.  We can recreate that here without having to code much at all (besides the HOG features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_magic = (1, -1.0, 0, 0.2, 1, 64, False)\n",
    "hog_d = cv2.HOGDescriptor((8,8),        # image size\n",
    "                        (8,8), (8,8), # frame size, frame steps\n",
    "                        (8,8),        # cell size\n",
    "                        9,            # number of bins \n",
    "                        *default_magic)\n",
    "\n",
    "def extract_features(images, hog_d=hog_d):\n",
    "    hists = [hog_d.compute(img).squeeze() for img in images]\n",
    "    data = np.stack(hists, 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%timeit -r1 gives ~ 4.41s \n",
    "# ~time to extract featuresm, build model, LOO cross-val\n",
    "# check last week's run time:  this is a lot slower ... but, sklearn has to deal with many\n",
    "# many more options that we didn't make use of ... and we have relatively small data\n",
    "# always take timings with a grain of salt\n",
    "data = extract_features(digits.images.astype(np.uint8))\n",
    "knn_classifier = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "preds = skms.cross_val_predict(knn_classifier, data, tgts, cv=skms.LeavePOut(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_helper(plt.gca(), tgts, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Is Simpler \"Better\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're currently doing a bit of comparing apples to oranges.  Our first example was a 3NN built on the raw images and our second was a 1NN built on HOG features.  Let's try a more direct comparison.  How does a 1NN fit to raw images do?  Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_examples = len(digits.images)\n",
    "data = digits.images.reshape(n_examples, -1)  # N rows, rest is flattened\n",
    "tgts = digits.target\n",
    "knn_classifier = neighbors.KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%timeit -r1  gives ~8.35 s\n",
    "# this takes a little while b/c we are repeatedly building a fairly large classifier\n",
    "# (recall KNN basically memorizes its data for later lookup)\n",
    "preds = skms.cross_val_predict(knn_classifier, data, tgts, cv=skms.LeavePOut(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# as is often the case, we have a trade off\n",
    "# in this case, we traded processing time for correctness\n",
    "#\n",
    "# also, if we dig into it, we will probably find that the majority of the time cost \n",
    "# in nearest neighbors in \"raw image space\" is mostly due to the size of the data table\n",
    "# (the images were 8x8 so we had Nx64 table; HOG space is \"only\" Nx9)\n",
    "#\n",
    "# also note:  our images are \"tiny\" so it is still feasible to work directly on pixels\n",
    "#       and:  these digits are very well aligned and evenly illuminated.  \n",
    "#             that gives an unfair advantage to kNN \n",
    "#             (from the POV of \"how well would it do in the real world?\")\n",
    "#  IF YOU WANT:  you could create pseudo-data by taking the \"nice\" digits and:\n",
    "#             1. translate and/or 2. rotate and/or 3. add noise/contrast variation\n",
    "#             then \"presto\" you have more realistic data without having to manually go out\n",
    "#             and get more data\n",
    "cm_helper(plt.gca(), tgts, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to CV and a Better Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloading data so we have a one-cell example:\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# simplify some names and massage shape of data\n",
    "n_examples = len(digits.images)\n",
    "data = digits.images.reshape(n_examples, -1)  # N rows, rest is flattened\n",
    "tgts = digits.target\n",
    "\n",
    "# create and fit model\n",
    "# support-vector-classifier, c-formulation, rbf kernel (super powers!)\n",
    "# gamma:  how far does an example reach:\n",
    "#         small neighborhood in space (big value of gamma)\n",
    "#         or big neighborhood (small value)\n",
    "#         it operates like the inverse variance (precision) of a normal distribution\n",
    "svc = svm.SVC(gamma=0.001) \n",
    "\n",
    "# StratifiedKFold means we want \n",
    "# balanced representation between each class [0-9] in each fold\n",
    "predicted = skms.cross_val_predict(svc, data, tgts, cv=skms.StratifiedKFold())\n",
    "cm_helper(plt.gca(), tgts, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note, this only downloads if the data is missing from data_home\n",
    "lfw_people = datasets.fetch_lfw_people(min_faces_per_person=70, \n",
    "                                       resize=0.4,\n",
    "                                       data_home=\"./data/skl_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Shape Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lfw_people has two views of the underlying information\n",
    "# .data -> \"flat\" one row per image, h*w features\n",
    "# .images -> each image as an (h,w) matrix\n",
    "\n",
    "# we'll make use of the image shapes for display purposes below\n",
    "n_samples, *img_shape = lfw_people.images.shape\n",
    "n_features = np.prod(img_shape)\n",
    "n_classes  = len(lfw_people.target_names)\n",
    "\n",
    "print(n_classes, n_samples, n_features, img_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "faces  = lfw_people.data\n",
    "faces_target = lfw_people.target\n",
    "\n",
    "(faces_train, faces_test, \n",
    " faces_train_tgt, faces_test_tgt) = skms.train_test_split(faces, faces_target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Pre\" PCA followed by SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing with PCA:  fit and transform the training data\n",
    "n_components = 150\n",
    "pca = decomposition.PCA(n_components=n_components, svd_solver='randomized', whiten=True)\n",
    "pca_faces_train = pca.fit_transform(faces_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines are a major player in the machine learning community.  There are a number of different, more or less equivalent, formulations of them.  In the formulation we are using through scikit-learn, there are two primary parameters that we care about.  $\\gamma$ (gamma) controls how far the influence of a single example can spread.  You can think of it like variance in a Gaussian (normal) distribution:  a bigger variance makes the connection between the mean and a point far away more likely.  However, $\\gamma$ works like the inverse of variance (sometimes called precision).  $C$ controls a fundamental tradeoff between training-accuracy and model-simplicity (bias-variance and over/underfitting).  A high value of $C$ allows the model to \"wiggle more\" to classify the training data better.  This comes at the risk of overfitting noisy datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define and fit the main model\n",
    "param_grid = {'C'     : np.logspace(-2, 4, 7), \n",
    "              'gamma' : np.logspace(-4, -1, 4)}\n",
    "\n",
    "grid_svc = skms.GridSearchCV(svm.SVC(class_weight='balanced'), param_grid)\n",
    "model    = grid_svc.fit(pca_faces_train, faces_train_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict on the test data\n",
    "faces_pred = model.predict(pca.transform(faces_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parameters\", model.best_params_)\n",
    "print(metrics.classification_report(faces_test_tgt, faces_pred, \n",
    "                                    target_names=lfw_people.target_names))\n",
    "# does this need to be fixed?\n",
    "cm = metrics.confusion_matrix(faces_test_tgt, faces_pred, \n",
    "                              labels=range(n_classes))\n",
    "ax = sns.heatmap(cm, fmt=\"3d\", annot=True)\n",
    "ax.set_ylabel('expected')\n",
    "ax.set_xlabel('predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define and fit the pipeline model\n",
    "param_grid = {'pca__n_components' : [25, 75, 150], # 150 is max for randomized solved\n",
    "              'svc__C'     : np.logspace(-2, 4, 7), \n",
    "              'svc__gamma' : np.logspace(-4, -1, 4)}\n",
    "\n",
    "pipe = pipeline.make_pipeline(decomposition.PCA(svd_solver='randomized',\n",
    "                                                whiten=True), \n",
    "                              svm.SVC(class_weight='balanced'))\n",
    "\n",
    "model = skms.GridSearchCV(pipe, param_grid, n_jobs=-1).fit(faces_train, faces_train_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict on the test data\n",
    "faces_pred = model.predict(faces_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parameters: \", model.best_params_)\n",
    "print(metrics.classification_report(faces_test_tgt, faces_pred, \n",
    "                                    target_names=lfw_people.target_names))\n",
    "cm = metrics.confusion_matrix(faces_test_tgt, faces_pred, labels=range(n_classes))\n",
    "\n",
    "ax = sns.heatmap(cm, fmt=\"3d\", annot=True)\n",
    "ax.set_ylabel('expected')\n",
    "ax.set_xlabel('predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idx_to_names(idx):\n",
    "    return [n.rsplit(None, 1)[-1] for n in lfw_people.target_names[idx]]\n",
    "real_names = idx_to_names(faces_test_tgt[:10]) # lfw_people.target_names[faces_test_tgt]\n",
    "pred_names = idx_to_names(faces_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_images = faces_test[:10].reshape(-1, *img_shape)\n",
    "fig, axes = plt.subplots(2,5,figsize=(10,4))\n",
    "for ax, img, real, pred in zip(axes.flat, eval_images, real_names, pred_names):\n",
    "    my_gshow(ax, img)\n",
    "    ax.set_title(\"Actual {}\\nPredict {}\".format(real, pred))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Eigenfaces (Happy Halloween!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenfaces = pca.components_.reshape(n_components, *img_shape)\n",
    "fig, axes = plt.subplots(2,5,figsize=(10,4))\n",
    "for idx, (ax, ef) in enumerate(zip(axes.flat, eigenfaces), 1):\n",
    "    my_gshow(ax, ef)\n",
    "    ax.set_title(\"Eigenface {}\".format(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Complicated Features and a Different Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The caltech Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 101_ObjectCategories from:\n",
    "# \"http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz\"\n",
    "# extracted airplanes, butterfly, panda, grand_piano, dollar_bill\n",
    "# to data/data/101_ObjectCategories/airplanes/*\n",
    "#                                   etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Visual Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# object recognition:  is an object (a cat) in an image\n",
    "# object detection:  where is the object within an image (get a bounding box)\n",
    "\n",
    "# we are doing recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/101_ObjectCategories\"\n",
    "obj_classes = [osp.split(d)[-1] for d in glob.glob(osp.join(data_path, \"*\"))]\n",
    "print(obj_classes)\n",
    "\n",
    "def make_paths(obj_class):\n",
    "    form = osp.join(data_path, obj_class, \"*\")\n",
    "    img_paths = glob.glob(form)\n",
    "    num_imgs  = len(img_paths)\n",
    "    return img_paths[:20]\n",
    "\n",
    "# read images into a dictionary of object class : [list of images]\n",
    "loi = [(oc,[my_read(p) for p in make_paths(oc)]) for oc in obj_classes]\n",
    "training_imgs = co.OrderedDict(loi)\n",
    "\n",
    "# calculate these now for future use\n",
    "numbered_images = enumerate(training_imgs.values())\n",
    "image_labels    = list(it.chain.from_iterable([idx] * len(v) for idx, v in numbered_images))\n",
    "num_images      = len(image_labels)\n",
    "\n",
    "class_labels = dict((x,y) for y,x in enumerate(obj_classes))\n",
    "\n",
    "print(len(training_imgs['airplanes']))\n",
    "print(training_imgs['airplanes'][0].shape)\n",
    "print('number of images:', num_images)\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "for idx, k in enumerate(training_imgs.keys()):\n",
    "    print(idx, k, len(training_imgs[k]))\n",
    "    end = start + len(training_imgs[k])\n",
    "    assert all(i==class_labels[k] for i in image_labels[start:end]), image_labels[start:end]  \n",
    "    start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need list of features coming from each class\n",
    "def map_dict(src, f):\n",
    "    ' helper to apply f to each elt of the lists in the values of src (a dict)'\n",
    "    #return {k:f(v) for k,v in src.items()}\n",
    "    return co.OrderedDict((k,f(v)) for k,v in src.items())\n",
    "    \n",
    "#src = {1:'red', 2:'cat'}\n",
    "#def f(v):\n",
    "#    return(len(v))\n",
    "#map_dict(src, f)\n",
    "#{1: 3, 2: 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Descriptors From Each Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIXME:  use orb instead?\n",
    "def get_sift_desc_one(img):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    key_points, descriptors = sift.detectAndCompute(img, None)\n",
    "    return descriptors\n",
    "get_sift_desc_lst = ft.partial(map, get_sift_desc_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lazy_desc = map_dict(training_imgs, get_sift_desc_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't have to do this\n",
    "eager_desc = map_dict(lazy_desc, list)\n",
    "print(eager_desc['airplanes'][3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eager_desc['airplanes'] # 40 airplane descriptors\n",
    "# shapes of descriptors for first two airplane images; \n",
    "# each has 128 \"learning-features-columns\"\n",
    "eager_desc['airplanes'][0].shape, eager_desc['airplanes'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a single array of *all* of the descriptors over each object\n",
    "all_desc = list(it.chain.from_iterable(eager_desc.values()))\n",
    "# 5 * 40 - 2 b/c only 38 total pandas\n",
    "print(len(all_desc), all_desc[0].shape)\n",
    "print(all(d.shape[1] == 128 for d in all_desc))\n",
    "all_desc = np.concatenate(all_desc, 0)\n",
    "all_desc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster the Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusters = 20\n",
    "km_clusterer = cluster.KMeans(n_clusters=n_clusters)\n",
    "desc_clusters = km_clusterer.fit_predict(all_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one cluster is a \"visual word\"\n",
    "# represent each image as a combination of visual-words\n",
    "#      frequency of visual words in each image\n",
    "# \n",
    "# vocabulary is histogram of all visual-words over all images\n",
    "\n",
    "# mapping from *a descriptor* to its cluster\n",
    "desc_clusters[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is superbly annoying, but it gets a sequences of \n",
    "# unique image labels for the *entire* image database\n",
    "# so we can tell what descriptors came from what images\n",
    "# see next cell for some explanation ...\n",
    "img_labeled_descs = enumerate(it.chain.from_iterable(eager_desc.values()))\n",
    "res = list(it.chain.from_iterable([idx] * v.shape[0] for idx, v in img_labeled_descs))\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where do we flip from first airplane to second airplane\n",
    "num_descriptors = eager_desc['airplanes'][0].shape[0]\n",
    "print(res[num_descriptors-1], \n",
    "      res[num_descriptors], \n",
    "      res[num_descriptors+1])\n",
    "\n",
    "# where do descriptors start describing third airplane?\n",
    "base = num_descriptors\n",
    "num_descriptors = eager_desc['airplanes'][1].shape[0]\n",
    "print(num_descriptors)\n",
    "print(res[base+num_descriptors-1], \n",
    "      res[base+num_descriptors], \n",
    "      res[base+num_descriptors+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redescribe Images in Terms of Clusters \"Visual-Words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, we know descriptors -> clusters (visual words)\n",
    "# and we know  descriptors -> images\n",
    "# we need to describe images in terms of visual-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del eager_desc, all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = co.Counter(zip(res, desc_clusters))\n",
    "\n",
    "table = np.zeros((num_images, n_clusters))\n",
    "for (img, clust), count in counts.items():\n",
    "    table[img, clust] = count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn a Model from Redescribed Images to Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# column wise standardization is basically like normalizing the HOG histogram:\n",
    "# it says that the relative amounts in the bins/features are what matter\n",
    "# not the individual magnitudes of those amounts\n",
    "# (related to how eigenvectors capture the orientation of a matrix and \n",
    "#  eigenvalues capture the scale -- here, we care about just the orientation)\n",
    "\n",
    "std_svc = pipeline.make_pipeline(skpre.StandardScaler(), svm.SVC())\n",
    "svc = std_svc.fit(table, image_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Using Learned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction Pipeline for One Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image -> features \n",
    "#       -> clusters for features \n",
    "#       -> cluster representation (Clusters of Counts aka Bag of Words)\n",
    "#       -> normalized+svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image\n",
    "test_image = my_read(osp.join(data_path, \"airplanes/image_0700.jpg\"))\n",
    "\n",
    "# --> features\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "_, test_descs = sift.detectAndCompute(test_image, None)\n",
    "\n",
    "# --> clusters\n",
    "test_desc_clusters = km_clusterer.predict(test_descs)\n",
    "\n",
    "# --> histogram\n",
    "test_cluster_counts = co.Counter(test_desc_clusters)\n",
    "\n",
    "# (hack it into a pseudo-row for sklearn happiness)\n",
    "test_pseudo_row = np.zeros((1,20))\n",
    "for clust, count in test_cluster_counts.items():\n",
    "    test_pseudo_row[0,clust] = count\n",
    "\n",
    "# prediction\n",
    "#print(test_pseudo_row)\n",
    "svc.predict(test_pseudo_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_paths(obj_class):\n",
    "    form = osp.join(data_path, obj_class, \"*\")\n",
    "    img_paths = glob.glob(form)\n",
    "    num_imgs  = len(img_paths)\n",
    "    return img_paths[-5:]\n",
    "\n",
    "def image_to_example(path):\n",
    "    ' functional form of image (via filename) --> COW '\n",
    "    test_image = my_read(path) \n",
    "\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    _, test_descs = sift.detectAndCompute(test_image, None)\n",
    "\n",
    "    test_desc_clusters = km_clusterer.predict(test_descs)\n",
    "\n",
    "    test_cluster_counts = co.Counter(test_desc_clusters)\n",
    "    test_pseudo_row = np.zeros((1,20))\n",
    "    for clust, count in test_cluster_counts.items():\n",
    "        test_pseudo_row[0,clust] = count\n",
    "    \n",
    "    return test_pseudo_row\n",
    "\n",
    "results = []\n",
    "for oc in obj_classes:\n",
    "    expected  = class_labels[oc]\n",
    "    print(oc, expected)\n",
    "    for test_img_path in make_test_paths(oc):\n",
    "        example = image_to_example(test_img_path)\n",
    "        predicted = svc.predict(example)[0]\n",
    "        results.append((expected, predicted))\n",
    "results = np.array(results, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_helper(plt.gca(), results[:,0], results[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interactions between Learners and Features (Descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's compare svm and knn.  We used hog+knn ... let's use a good representation (hog) and a stronger learner (svm).  Use stratified CV to compare the following setups on the digits dataset:\n",
    "\n",
    " * svm, knn on raw images\n",
    " * svm, knn on hog features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_magic = (1, -1.0, 0, 0.2, 1, 64, True)\n",
    "hog_d = cv2.HOGDescriptor((8,8),        # image size\n",
    "                          (8,8), (8,8), # frame size, frame steps\n",
    "                          (8,8),        # cell size\n",
    "                          18,            # number of bins \n",
    "                          *default_magic)\n",
    "\n",
    "def extract_hog_features(images, hog_d=hog_d):\n",
    "    hists = [hog_d.compute(img).squeeze() for img in images]\n",
    "    data = np.stack(hists, 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "n_examples = len(digits.images)\n",
    "data = digits.images.reshape(n_examples, -1)  # N rows, rest is flattened\n",
    "tgts = digits.target\n",
    "\n",
    "methods = [('knn(3)', neighbors.KNeighborsClassifier(n_neighbors=1)),\n",
    "            ('svm',    svm.SVC(gamma=0.001))]\n",
    "\n",
    "hog_data = extract_hog_features(digits.images.astype(np.uint8))\n",
    "\n",
    "predicted = {}\n",
    "for (name, m), use_hog in it.product(methods, [False, True]):\n",
    "    my_data = hog_data if use_hog else data\n",
    "    predicted[name, use_hog] = skms.cross_val_predict(m, my_data, tgts, \n",
    "                                                      cv=skms.StratifiedKFold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(10,10))\n",
    "\n",
    "for ax, cnds in zip(axes.flat, predicted):\n",
    "    acc = metrics.accuracy_score(digits.target, predicted[cnds])\n",
    "    \n",
    "    cm = metrics.confusion_matrix(digits.target, predicted[cnds])\n",
    "    sns.heatmap(cm, annot=True, fmt='3d', ax=ax)\n",
    "    \n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_xlabel('Predicted');\n",
    "    ax.set_title(\"Method,UseHog={}\\nAcc={:5.4f}\".format(cnds, acc))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on the terrible results with SVM and HOG.  The svm we are using has a \"Gaussian radial basis function\" kernel.  There is deep mathematics beneath it, but one consideration when using it is that it prefers data that is centered (column-wise) around 0 and has variances that are comparable.  You might like to redo the svm/hog example with a pipeline that incorporates a `StandardScaler` (as in the Bag-of-visual-Words example).  Does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EigenDigits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Can we construct \"eigendigits\" (much like we constructed eigenface?  Yes.  Does it help?  Let's find out!  Work through the eigenfaces example, but using the digits data instead.  At the end, display the top 10 eigendigits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits_dataset = datasets.load_digits()\n",
    "n_examples, *img_shape = digits_dataset.images.shape\n",
    "\n",
    "digits = digits_dataset.images.reshape(n_examples, -1)\n",
    "digits_target = digits_dataset.target\n",
    "\n",
    "(digits_train,     digits_test, \n",
    " digits_train_tgt, digits_test_tgt) = skms.train_test_split(digits, digits_target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define and fit the pipeline model\n",
    "param_grid = {'pca__n_components' : [16, 32, 48],\n",
    "              'svc__C'     : np.logspace(-2, 4, 7), \n",
    "              'svc__gamma' : np.logspace(-4, -1, 4)}\n",
    "\n",
    "pipe = pipeline.make_pipeline(decomposition.PCA(svd_solver='randomized',\n",
    "                                                whiten=True), \n",
    "                              svm.SVC(class_weight='balanced'))\n",
    "\n",
    "model = skms.GridSearchCV(pipe, param_grid, n_jobs=-1).fit(digits_train, digits_train_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_preds = model.predict(digits_test)\n",
    "\n",
    "print(\"Parameters: \", model.best_params_)\n",
    "print(metrics.classification_report(digits_test_tgt, digits_preds))\n",
    "cm_helper(plt.gca(), digits_test_tgt, digits_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = model.best_estimator_.named_steps['pca'].n_components\n",
    "eigendigits = (model.best_estimator_.named_steps['pca']\n",
    "               .components_.reshape(n_components, *img_shape))\n",
    "fig, axes = plt.subplots(2,5,figsize=(10,4))\n",
    "for ax, ed in zip(axes.flat, eigendigits):\n",
    "    my_gshow(ax, ed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vocabulary Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Does enlarging or reducing our vocabulary (the number of clusters)  help with the BOW approach?  Try working through the BOW example with different vocabulary sizes.  While you're at it, you might want to turn the example code above into a few functions to support building the vocabulary and extracting features from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_paths(obj_class):\n",
    "    form = osp.join(\"data/101_ObjectCategories\", obj_class, \"*\")\n",
    "    img_paths = glob.glob(form)\n",
    "    num_imgs  = len(img_paths)\n",
    "    return img_paths[:20]\n",
    "\n",
    "def map_dict(src, f):\n",
    "    return co.OrderedDict((k,f(v)) for k,v in src.items())\n",
    "\n",
    "def create_vocab_and_training_data(img_paths, vocab_size=20):\n",
    "    # calculate these now for future use\n",
    "    numbered_images = enumerate(img_paths.values())\n",
    "    image_labels    = list(it.chain.from_iterable([idx] * len(v) for idx, v in numbered_images))\n",
    "    num_images      = len(image_labels)\n",
    "    \n",
    "    def get_sift_desc_one(img):\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        key_points, descriptors = sift.detectAndCompute(img, None)\n",
    "        return descriptors\n",
    "    get_sift_desc_lst = ft.partial(map, get_sift_desc_one)\n",
    "\n",
    "    # (*) here and (*) below can be improved upon\n",
    "    lazy_desc = map_dict(img_paths, get_sift_desc_lst)\n",
    "    eager_desc = map_dict(lazy_desc, list)\n",
    "    \n",
    "    # need a single array of *all* of the descriptors over each object\n",
    "    all_desc = list(it.chain.from_iterable(eager_desc.values()))\n",
    "    all_desc = np.concatenate(all_desc, 0)\n",
    "\n",
    "    km_clusterer = cluster.KMeans(n_clusters=vocab_size)\n",
    "    desc_clusters = km_clusterer.fit_predict(all_desc)\n",
    "\n",
    "    # (*) here and (*) above can be improved upon\n",
    "    img_labeled_descs = enumerate(it.chain.from_iterable(eager_desc.values()))\n",
    "    res = list(it.chain.from_iterable([idx] * v.shape[0] for idx, v in img_labeled_descs))\n",
    "\n",
    "    # now, we know descriptors -> clusters (visual words)\n",
    "    # and we know  descriptors -> images\n",
    "    del eager_desc, all_desc\n",
    "\n",
    "\n",
    "    counts = co.Counter(zip(res, desc_clusters))\n",
    "    table = np.zeros((num_images, vocab_size))\n",
    "    for (img, clust), count in counts.items():\n",
    "        table[img, clust] = count\n",
    "    \n",
    "    return km_clusterer, table, image_labels\n",
    "\n",
    "def make_test_paths(obj_class):\n",
    "    form = osp.join(\"101_ObjectCategories\", obj_class, \"*\")\n",
    "    img_paths = glob.glob(form)\n",
    "    num_imgs  = len(img_paths)\n",
    "    return img_paths[-5:]\n",
    "\n",
    "def image_to_example(path, vocab, vocab_size=20):\n",
    "    test_image = my_read(path) \n",
    "\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    _, test_descs = sift.detectAndCompute(test_image, None)\n",
    "\n",
    "    test_desc_clusters = vocab.predict(test_descs)\n",
    "\n",
    "    test_cluster_counts = co.Counter(test_desc_clusters)\n",
    "    test_pseudo_row = np.zeros((1,vocab_size))\n",
    "    for clust, count in test_cluster_counts.items():\n",
    "        test_pseudo_row[0,clust] = count    \n",
    "    return test_pseudo_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read images into a dictionary of class : [list of images]\n",
    "obj_classes = [osp.split(d)[-1] for d in glob.glob(\"data/101_ObjectCategories/*\")]\n",
    "training_img_paths = co.OrderedDict((oc,[my_read(p) for p in make_paths(oc)]) \n",
    "                                                    for oc in obj_classes) # outer loop\n",
    "\n",
    "# create vocabulary and training data\n",
    "vocab_size = 20\n",
    "vocab, training_data, labels = create_vocab_and_training_data(training_img_paths, vocab_size)\n",
    "\n",
    "# build learning model\n",
    "std_svc = pipeline.make_pipeline(skpre.StandardScaler(), svm.SVC())\n",
    "svc = std_svc.fit(training_data, labels)\n",
    "\n",
    "class_labels = dict((x,y) for y,x in enumerate(obj_classes))\n",
    "\n",
    "results = []\n",
    "for oc in obj_classes:\n",
    "    expected  = class_labels[oc]\n",
    "    for test_img_path in make_test_paths(oc):\n",
    "        example = image_to_example(test_img_path, vocab, vocab_size)\n",
    "        predicted = svc.predict(example)[0]\n",
    "        results.append((expected, predicted))\n",
    "results = np.array(results, dtype=np.uint8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 OpenCV3 (Forge)",
   "language": "python",
   "name": "opencv-forge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
