{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import functools as ft\n",
    "import itertools as it\n",
    "import operator as op\n",
    "\n",
    "from utilities import my_show, my_gshow, my_read, my_read_g, my_read_cg\n",
    "\n",
    "img_dir = '../common/'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flat space is hard to localize\n",
    "# edges are hard to localize along the edge\n",
    "# corners are \"easy\" to localize\n",
    "\n",
    "# regions when moved cause \"maximal variation\" ... \n",
    "# i.e., small jitter on a blob/corner results in lots of differences\n",
    "#       even big jitter on blue sky background has lots of similarities\n",
    "# finding these is \"feature detection\"\n",
    "\n",
    "# building a \"good context\" around a feature is \"feature description\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = np.zeros((15, 15), dtype=np.int8)\n",
    "box[4:11, 4:11] = 1.0\n",
    "\n",
    "fig, axes = plt.subplots(2,3, figsize=(9,6))\n",
    "\n",
    "def show_asis(ax, arr, title):  \n",
    "    ' custom display; using colors to show boundaries '\n",
    "    ax.imshow(arr, vmin=0, vmax=1, interpolation='none')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "show_asis(axes[0,0], box,           'whole')\n",
    "show_asis(axes[0,1], box[:4, :4],   'back')\n",
    "show_asis(axes[0,2], box[5:9, 5:9], 'fore')\n",
    "\n",
    "show_asis(axes[1,0], box[5:9, 2:6], 'l-edge')\n",
    "show_asis(axes[1,1], box[2:6, 5:9], 't-edge')\n",
    "\n",
    "show_asis(axes[1,2], box[2:6, 2:6], 'corner')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harris Corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a chessboard\n",
    "base_len = 100\n",
    "black = np.zeros((base_len, base_len), dtype=np.uint8)\n",
    "white = np.zeros((base_len, base_len), dtype=np.uint8) + 255\n",
    "\n",
    "top_left = np.c_[np.r_[white,black], np.r_[black,white]]\n",
    "top_left.shape\n",
    "\n",
    "board = np.tile(top_left, (4,4))\n",
    "\n",
    "my_gshow(plt.gca(), board, interpolation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3,figsize=(21,12))\n",
    "\n",
    " # block size, aperature in sobel, coeff in Harris\n",
    "corners = cv2.cornerHarris(board.astype('float32'), 2, 3, 0.04)\n",
    "my_gshow(axes[0], corners[50:150,50:150])\n",
    "\n",
    "\n",
    "#result is dilated for marking the corners, not important\n",
    "corners = cv2.dilate(corners,None)\n",
    "my_gshow(axes[1], corners[50:150, 50:150])\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "cboard = cv2.cvtColor(board, cv2.COLOR_GRAY2RGB)\n",
    "cboard[corners>0.01*corners.max()] = [255,0,0]\n",
    "my_show(axes[2], cboard[50:150, 50:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,x = r,c = board.shape\n",
    "pts1 = np.float32([[0,0], [x,0], [0,y], [x,y]])\n",
    "pts2 = np.float32([[base_len*2,0], [x-(base_len*2), 0], [0,y], [x,y]])\n",
    "\n",
    "M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "skewed = cv2.warpPerspective(board, M, board.shape, \n",
    "                             # flags=cv2.INTER_NEAREST, \n",
    "                             borderValue=256)\n",
    "\n",
    "my_gshow(plt.gca(), skewed, interpolation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3,figsize=(15,9))\n",
    "\n",
    "# args are block size, aperature in sobel, coeff in Harris\n",
    "corners = cv2.cornerHarris(skewed.astype(np.float32), 2, 3, 0.04)\n",
    "my_gshow(axes[0], corners)\n",
    "\n",
    "\n",
    "#result is dilated for marking the corners, not important\n",
    "corners = cv2.dilate(corners,None)\n",
    "my_gshow(axes[1], corners)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "cskewed = cv2.cvtColor(skewed, cv2.COLOR_GRAY2RGB)\n",
    "cskewed[corners>0.01*corners.max()] = [255,0,0]\n",
    "my_show(axes[2], cskewed[:250, :250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributing Corners to Locations Below Pixel Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high accuracy corners\n",
    "fig, axes = plt.subplots(1,4,figsize=(12,8))\n",
    "\n",
    "my_gshow(axes[0], board[90:110, 90:110], interpolation=None)\n",
    "\n",
    "\n",
    "corners = cv2.cornerHarris(board.astype(np.float32), 2, 3, 0.04) # block size, aperature in sobel, coeff in Harris\n",
    "\n",
    "#result is dilated for marking the corners, not important\n",
    "corners = cv2.dilate(corners,None)\n",
    "corners = cv2.threshold(corners, 0.01*corners.max(), 255, 0)[1].astype(np.uint8)\n",
    "my_gshow(axes[1], corners[90:110, 90:110], interpolation=None)\n",
    "\n",
    "\n",
    "# find centroids\n",
    "ret, labels, stats, centroids = cv2.connectedComponentsWithStats(corners)\n",
    "\n",
    "# define the criteria to stop and refine the corners\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n",
    "fine_corners = cv2.cornerSubPix(board.astype(np.float32), np.float32(centroids), \n",
    "                                (5,5), (-1,-1), criteria)\n",
    "\n",
    "centroids, fine_corners = centroids.astype(np.int32), fine_corners.astype(np.int32)\n",
    "\n",
    "# draw located corners on board (color version)\n",
    "cboard = cv2.cvtColor(board, cv2.COLOR_GRAY2RGB)\n",
    "r,c = centroids[:,1], centroids[:,0]  # x,y --> r,c\n",
    "cboard[r,c] = [255,0,0]\n",
    "my_show(axes[2], cboard[90:110, 90:110], interpolation=None)\n",
    "\n",
    "\n",
    "r,c = fine_corners[:,1], fine_corners[:,0]\n",
    "cboard[r,c] = [0,255,0]\n",
    "\n",
    "my_show(axes[3], cboard[90:110, 90:110], interpolation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features: Shi-Tomasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene, scene_g = my_read_cg(img_dir+'data/tsukuba.png')\n",
    "\n",
    "corners = cv2.goodFeaturesToTrack(scene_g,25,0.01,10).squeeze().astype(np.int32)\n",
    "print(corners.shape)\n",
    "\n",
    "[cv2.circle(scene, (x,y), 3, [255,0,0], -1) for x,y in corners] # -1 -> filled circle\n",
    "\n",
    "my_show(plt.gca(), scene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features:  Histogram-of-Oriented-Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# examples of code here:\n",
    "# https://www.learnopencv.com/histogram-of-oriented-gradients/\n",
    "# https://docs.opencv.org/3.2.0/dd/d3b/tutorial_py_svm_opencv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "print(dir(digits))\n",
    "print(digits.images.shape) # 1797x8x8 ---> 1797 images that are 8x8\n",
    "\n",
    "TEST_IMG = 172\n",
    "\n",
    "my_gshow(plt.gca(), digits.images[TEST_IMG], interpolation=None)\n",
    "#my_gshow(plt.gca(), digits.images[TEST_IMG], interpolation='bilinear')\n",
    "\n",
    "true_digit = digits.target_names[digits.target[TEST_IMG]]\n",
    "plt.gca().set_title(\"It's a {}\".format(true_digit));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy.linalg as nla\n",
    "\n",
    "def hog(img, unsigned=True, num_angle_bins=10, norm=True):\n",
    "    ' this is mini-hog that treats a small image as one unit cell '\n",
    "    # calculate gradients, convert to angle, magnitude (polar)\n",
    "    # convert radians (pi like) to degrees (180/360 like)\n",
    "    gx = cv2.Sobel(img, cv2.CV_64F, 1, 0)\n",
    "    gy = cv2.Sobel(img, cv2.CV_64F, 0, 1)\n",
    "    mag, ang = cv2.cartToPolar(gx, gy)\n",
    "    ang = (ang / 2*np.pi) * 360.0\n",
    "    if unsigned: # treat opposite directions the same\n",
    "        ang %= 180.0\n",
    "        upper_bin = 180.0\n",
    "    else:\n",
    "        upper_bin = 360.0\n",
    "\n",
    "    # create bins by angle\n",
    "    bins, bin_size = np.linspace(0, upper_bin, num_angle_bins, retstep=True)\n",
    "    n_bins   = bins.size\n",
    "    \n",
    "    # find proper neighbor bins for each angle \n",
    "    # (note, max - 180 or 360 - is still its own bin)\n",
    "    lwr_bins = np.uint8(ang // bin_size)\n",
    "    upr_bins = np.uint8(lwr_bins + 1)\n",
    "    \n",
    "    # weight by angle <-> bin-center distance\n",
    "    lwr_contrib = (ang - bins[lwr_bins]) / bin_size\n",
    "    upr_contrib = (bins[upr_bins] - ang) / bin_size\n",
    "\n",
    "    # place upper most bin (180 or 360) into 0 bin\n",
    "    upr_bins[upr_bins == n_bins-1] = 0\n",
    "    \n",
    "    # add.at is like += but will keep adding repeated index values\n",
    "    acc = np.zeros(n_bins - 1)\n",
    "    np.add.at(acc, lwr_bins, lwr_contrib*mag)\n",
    "    np.add.at(acc, upr_bins, upr_contrib*mag)\n",
    "    # alternative to add.at\n",
    "    # acc =  np.bincount(lwr_bins, lwr_contrib, minlength=n_bins)\n",
    "    # acc += np.bincount(upr_bins, upr_contrib, minlength=n_bins)\n",
    "    \n",
    "    \n",
    "    # normalize the vector so it's length (sqrt(a**2 + b**2 ... etc.)) is 1.0\n",
    "    if norm:\n",
    "        acc /= nla.norm(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3)\n",
    "my_gshow(axes[0], digits.images[TEST_IMG], interpolation=None)\n",
    "\n",
    "this_hog = hog(digits.images[TEST_IMG], norm=False)\n",
    "axes[1].hist(range(9), weights=this_hog)\n",
    "axes[1].set_title(\"Unnormalized\\nUnsigned\\nHistogram of Gradients\")\n",
    "\n",
    "this_hog = hog(digits.images[TEST_IMG])\n",
    "axes[2].hist(range(9), weights=this_hog)\n",
    "axes[2].set_title(\"Unsigned\\nHistogram of Gradients\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hog(digits.images[TEST_IMG]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are almost undocumented; the only one we care about is\n",
    "# the last argument (use directionality) False \n",
    "default_magic = (1, -1.0, 0, 0.2, 1, 64, False)\n",
    "hog_d = cv2.HOGDescriptor((8,8),        # image size\n",
    "                        (8,8), (8,8), # frame size, frame steps\n",
    "                        (8,8),        # cell size\n",
    "                        9,            # number of bins \n",
    "                        *default_magic)\n",
    "hist = hog_d.compute(digits.images[TEST_IMG].astype(np.uint8))\n",
    "print(hist.flatten()) \n",
    "# pretty different -- no real way to compare short of reading the opencv source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, in \"full\" HOG, there are two more layers of processing:\n",
    "  * the normalization we did on line 41 ( `acc /= nla.norm(acc)` ) is applied over a frame of cells [we had the equivalent of one cell above]\n",
    "  * when used for object detection (finding an object in an image) the full image will be broken into many smaller regions-of-interest that will be evaluated with HOG and compared with the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# within gaussian pyramid:\n",
    "# look at \"maxima\" wrt space (across image) and scale (up-and-down DoG pyramid)\n",
    "# size of circle is related to the level of Gaussian pyramid at which the keypoint was found\n",
    "venice, venice_g = my_read_cg(img_dir+'data/venice.jpg')\n",
    "key_points = cv2.xfeatures2d.SIFT_create().detect(venice_g, None)\n",
    "\n",
    "# venice = cv2.drawKeypoints(venice, key_points, venice)\n",
    "venice = cv2.drawKeypoints(venice, key_points, venice, \n",
    "                           flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "my_show(plt.gca(), venice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# detect finds keypoints\n",
    "# compute calculates the descriptors for those keypoints\n",
    "# detectAndCompute does both in one call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "venice, venice_g = my_read_cg(img_dir+'data/venice.jpg')\n",
    "key_points, descriptors = cv2.xfeatures2d.SIFT_create().detectAndCompute(venice_g, None)\n",
    "# desc is shape:  (kp, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features:  SURF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venice, venice_g = my_read_cg(img_dir+'data/venice.jpg')\n",
    "key_points = cv2.xfeatures2d.SURF_create().detect(venice_g, None)\n",
    "print(len(key_points))\n",
    "\n",
    "# FIXME what is the tradeoff of drawing on gray versus full-color\n",
    "venice = cv2.drawKeypoints(venice, key_points, venice, \n",
    "                           flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "my_show(plt.gca(), venice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surf likes blobs\n",
    "venice, venice_g = my_read_cg(img_dir+'data/venice.jpg')\n",
    "surf = cv2.xfeatures2d.SURF_create()\n",
    "\n",
    "print(surf.getHessianThreshold())\n",
    "surf.setHessianThreshold(5000)\n",
    "\n",
    "key_points = surf.detect(venice_g, None)\n",
    "print(len(key_points))\n",
    "\n",
    "venice_g = cv2.drawKeypoints(venice_g, key_points, venice_g, \n",
    "                             flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "my_show(plt.gca(), venice_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venice, venice_g = my_read_cg(img_dir+'data/venice.jpg')\n",
    "surf = cv2.xfeatures2d.SURF_create()\n",
    "\n",
    "surf.setHessianThreshold(5000)\n",
    "surf.setUpright(True)\n",
    "key_points = surf.detect(venice_g, None)\n",
    "print(len(key_points))\n",
    "\n",
    "venice_g = cv2.drawKeypoints(venice_g, key_points, venice_g, \n",
    "                             flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "my_show(plt.gca(), venice_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venice, venice_g = my_read_cg(img_dir+'data/venice.jpg')\n",
    "surf = cv2.xfeatures2d.SURF_create()\n",
    "\n",
    "surf.setHessianThreshold(5000)\n",
    "surf.setUpright(True)\n",
    "surf.setExtended(True) # 128 length descriptors\n",
    "\n",
    "# detectAndCompute to get keypoints and descriptors at once\n",
    "key_points, descriptors = surf.detectAndCompute(venice_g, None)\n",
    "print(len((key_points)))\n",
    "\n",
    "venice_g = cv2.drawKeypoints(venice_g, key_points, venice_g, \n",
    "                             flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "my_show(plt.gca(), venice_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features:  FAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venice, venice_g = my_read_cg(img_dir+'data/venice.jpg')\n",
    "\n",
    "fig, axes = plt.subplots(1,3,figsize=(12,8))\n",
    "my_show(axes[0], venice)\n",
    "\n",
    "fast = cv2.FastFeatureDetector_create()\n",
    "\n",
    "# find and draw the keypoints\n",
    "key_points = fast.detect(venice, None)\n",
    "print(fast.getThreshold(), fast.getNonmaxSuppression(), fast.getType(), len(key_points))\n",
    "venice_kp = cv2.drawKeypoints(venice.copy(), key_points, None, color=(255,0,0))\n",
    "my_show(axes[1], venice_kp)\n",
    "\n",
    "\n",
    "fast.setNonmaxSuppression(False)\n",
    "key_points = fast.detect(venice, None)\n",
    "print(fast.getThreshold(), fast.getNonmaxSuppression(), fast.getType(), len(key_points))\n",
    "venice_kp = cv2.drawKeypoints(venice.copy(), key_points, None, color=(255,0,0))\n",
    "my_show(axes[2], venice_kp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features:  Descriptors with BRIEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# often descriptors are converted to strings for use with hamming coding\n",
    "# brief goes directly to ham-able descriptors\n",
    "venice = my_read(img_dir+'data/venice.jpg')\n",
    "\n",
    "# Star = CenSurE (???) & brief\n",
    "star = cv2.xfeatures2d.StarDetector_create()\n",
    "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "\n",
    "# find the keypoints with STAR & descriptors with BRIEF\n",
    "key_points = star.detect(venice, None)\n",
    "key_points, descriptors = brief.compute(venice, key_points)\n",
    "print(len(key_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORB:  FAST + BRIEF without Patents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venice, venice_g = my_read_cg(img_dir+'data/venice.jpg')\n",
    "key_points = cv2.ORB_create().detect(venice_g, None)\n",
    "\n",
    "print(cv2.DRAW_MATCHES_FLAGS_DEFAULT, len(key_points))\n",
    "venice = cv2.drawKeypoints(venice, key_points, venice)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "my_show(plt.gca(), venice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Features:  Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brute Force Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First ten (not necessarily best 10!) matches found using brute force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = my_read_g(img_dir+'data/box.png')\n",
    "scene = my_read_g(img_dir+'data/box_in_scene.png')\n",
    "\n",
    "# find the keypoints and descriptors with ORB\n",
    "orb = cv2.ORB_create()\n",
    "key_points_box,   descriptors_box   = orb.detectAndCompute(box,None)\n",
    "key_points_scene, descriptors_scene = orb.detectAndCompute(scene,None)\n",
    "\n",
    "# the split .().() calls look odd, but it is gaining traction in python land\n",
    "# (it comes more from java/C++ land)\n",
    "matches = (cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "              .match(descriptors_box, descriptors_scene))\n",
    "matches.sort(key=op.attrgetter('distance'))\n",
    "first_match = matches[0]\n",
    "print(first_match.distance,  # how similar are descriptors\n",
    "      first_match.trainIdx,  # which descriptor in source image?\n",
    "      first_match.queryIdx,  # which descriptor in tgt image?\n",
    "      first_match.imgIdx)    # which training image (if more than one)?\n",
    "\n",
    "\n",
    "# Draw first 10 matches.\n",
    "NO_SINGLES = cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "match_img = cv2.drawMatches(box, key_points_box, scene, key_points_scene, \n",
    "                            matches[:10], None, \n",
    "                            flags=NO_SINGLES)\n",
    "plt.figure(figsize=(15,10))\n",
    "my_show(plt.gca(), match_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brute force SIFT matches left after applying \"ratio test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = my_read_g(img_dir+'data/box.png')\n",
    "scene = my_read_g(img_dir+'data/box_in_scene.png')\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "key_points_box,   descriptors_box   = sift.detectAndCompute(box,None)\n",
    "key_points_scene, descriptors_scene = sift.detectAndCompute(scene,None)\n",
    "\n",
    "# only keep k=2 best matches per descriptor and then ratio test\n",
    "# this \"ratio test\" shows up in D. Lowe's original SIFT paper\n",
    "# so we mimic it here\n",
    "matches = cv2.BFMatcher().knnMatch(descriptors_box, descriptors_scene, k=2)\n",
    "# ugly: [m] ... drawMatchesKnn wants it\n",
    "matches = [[m] for m,n in matches if m.distance < 0.75 * n.distance]\n",
    "match_img = cv2.drawMatchesKnn(box, key_points_box, scene, key_points_scene, \n",
    "                               matches, None, \n",
    "                               flags=NO_SINGLES)\n",
    "plt.figure(figsize=(15,10))\n",
    "my_show(plt.gca(), match_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Matching using Approximate Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for sift/surf (if you want to experiment with it)\n",
    "# FLANN_INDEX_KDTREE = 1\n",
    "# index_params_siftsurf = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "\n",
    "# parameters for orb (we'll use orb here)\n",
    "# rec'd parameters in comments see here:\n",
    "# https://docs.opencv.org/3.0-beta/modules/flann/doc/flann_fast_approximate_nearest_neighbor_search.html#flann-index-t-index\n",
    "\n",
    "FLANN_INDEX_LSH = 6\n",
    "index_params_orb = dict(algorithm = FLANN_INDEX_LSH, \n",
    "                        table_number = 6,      # 12\n",
    "                        key_size = 12,         # 20\n",
    "                        multi_probe_level = 1) # 2\n",
    "\n",
    "# parameters for search process\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "\n",
    "\n",
    "box = my_read_g(img_dir+'data/box.png')\n",
    "scene = my_read_g(img_dir+'data/box_in_scene.png')\n",
    "\n",
    "# find the keypoints and descriptors with ORB\n",
    "orb = cv2.ORB_create()\n",
    "key_points_box,   descriptors_box   = orb.detectAndCompute(box, None)\n",
    "key_points_scene, descriptors_scene = orb.detectAndCompute(scene,None)\n",
    "\n",
    "# ugly:  Flann has randomization and it may return a singleton result\n",
    "#        seems to do that about 1/5 of the time\n",
    "#        so we loop until it returns valid results\n",
    "matches = [[]]\n",
    "while any(len(m) < 2 for m in matches):\n",
    "    matches = (cv2.FlannBasedMatcher(index_params_orb, search_params)\n",
    "                  .knnMatch(descriptors_box, descriptors_scene, k=2))\n",
    "matches = [[m] for m,n in matches if m.distance < 0.75 * n.distance] \n",
    "\n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    "                   singlePointColor = (255,0,0),\n",
    "                   flags = 0)\n",
    "match_img = cv2.drawMatchesKnn(box, key_points_box, \n",
    "                               scene, key_points_scene, \n",
    "                               matches, None, \n",
    "                               **draw_params)\n",
    "plt.figure(figsize=(15,10))\n",
    "my_show(plt.gca(), match_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features:  Homography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we find an alignment between src and target points, we can compute a homography (a perspective transformation) between the two images.  This has many applications:  among them, we could normalize for differences in the the perspective of cameras in different scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = [m[0] for m in matches] # undo ugliness\n",
    "\n",
    "# use points to generate a homography; findHomography expects Nx1x2 inputs\n",
    "# FIXME:  describe RANSAC\n",
    "box_points   = np.array([key_points_box[m.queryIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "scene_points = np.array([key_points_scene[m.trainIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "M, inliers = cv2.findHomography(box_points, scene_points, cv2.RANSAC, 5.0)\n",
    "inliers = inliers.ravel().tolist()\n",
    "\n",
    "# now we set up a box in our original (src) and we map it to an \n",
    "# M transformed box in the target (via perspective transform)\n",
    "r,c = box.shape\n",
    "src_bounds = np.array([[0  ,   0],\n",
    "                       [0  , r-1],\n",
    "                       [c-1, r-1],\n",
    "                       [c-1,   0]], dtype=np.float64).reshape(-1,1,2)\n",
    "dst_bounds = cv2.perspectiveTransform(src_bounds, M)[np.newaxis,:,:,:]\n",
    "\n",
    "# draw box around match in target scene\n",
    "out_scene = cv2.polylines(scene, dst_bounds.astype(np.int64), True, 255, 10, cv2.LINE_AA)\n",
    "\n",
    "# draw the points as well\n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = inliers, # draw only inlier connections\n",
    "                   flags = 0)\n",
    "\n",
    "match_img = cv2.drawMatches(box, key_points_box, \n",
    "                            out_scene, key_points_scene, \n",
    "                            matches, None, \n",
    "                            **draw_params)\n",
    "plt.figure(figsize=(15,10))\n",
    "my_show(plt.gca(), match_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined example of the above steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "box = my_read_g(img_dir+'data/box.png')\n",
    "scene = my_read_g(img_dir+'data/box_in_scene.png')\n",
    "\n",
    "# find the keypoints and descriptors with ORB\n",
    "orb = cv2.ORB_create()\n",
    "key_points_box,   descriptors_box   = orb.detectAndCompute(box,None)\n",
    "key_points_scene, descriptors_scene = orb.detectAndCompute(scene,None)\n",
    "\n",
    "# find key point matches using Flann\n",
    "matches = (cv2.FlannBasedMatcher(index_params_orb, search_params)\n",
    "              .knnMatch(descriptors_box, descriptors_scene, k=2))\n",
    "matches = [m for m,n in matches if m.distance < 0.75 * n.distance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use points to generate a homography; findHomography expects Nx1x2 inputs\n",
    "box_points   = np.array([key_points_box[m.queryIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "scene_points = np.array([key_points_scene[m.trainIdx].pt for m in matches]).reshape(-1,1,2)\n",
    "M,inliers = cv2.findHomography(box_points, scene_points, cv2.RANSAC, 5.0)\n",
    "inliers = inliers.ravel().tolist()\n",
    "\n",
    "# create box in source and target\n",
    "r,c = box.shape\n",
    "src_bounds = np.array([[0  ,   0],\n",
    "                       [0  , r-1],\n",
    "                       [c-1, r-1],\n",
    "                       [c-1,   0]],\n",
    "                       dtype=np.float64).reshape(-1,1,2)\n",
    "dst_bounds = cv2.perspectiveTransform(src_bounds, M)[np.newaxis,:,:,:]\n",
    "\n",
    "# draw box around box in target scene\n",
    "out_scene = cv2.polylines(scene, dst_bounds.astype(np.int64), True, 255, 10, cv2.LINE_AA)\n",
    "\n",
    "# draw points as well\n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = inliers,\n",
    "                   flags = NO_SINGLES)\n",
    "match_img = cv2.drawMatches(box, key_points_box, out_scene, key_points_scene, \n",
    "                            matches, None, **draw_params)\n",
    "plt.figure(figsize=(15,10))\n",
    "my_show(plt.gca(), match_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|      |PredP| PredN | |\n",
    "|------|-----|-----|-|\n",
    "|RealP | TP | FN  | TP/(TP+FN)<br>sensitivity, recall<br>TPR |\n",
    "|RealN | FP | TN  | TN/(FP+TN)<br>specificity<br>TNR |\n",
    "|      |TP/(TP+FP)<br>precision| |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Features:  A Naive, Handspun Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's make a super-simple-problem:\n",
    "# classify digits\n",
    "# we'll do it like this:  \n",
    "# 1:  convert digits to HOGs\n",
    "# 2:  find the closest HOG to me (that isn't me)\n",
    "# 3.  say that i'm whatever class that closest HOG is\n",
    "\n",
    "# note:  this is 1-nearest-neighbors with leave-one-out-cross-validation \n",
    "#        (train on everyone else, test on me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy.linalg as nla\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "\n",
    "default_magic = (1, -1.0, 0, 0.2, 1, 64, False)\n",
    "hog_d = cv2.HOGDescriptor((8,8),        # image size\n",
    "                          (8,8), (8,8), # frame size, frame steps\n",
    "                          (8,8),        # cell size\n",
    "                          9,            # number of (orientation) bins \n",
    "                          *default_magic)\n",
    "\n",
    "# hog_d=hog_d to: \n",
    "#  (1) not recreate every time and \n",
    "#  (2) not use global variable\n",
    "def extract_features(images, hog_d=hog_d):\n",
    "    hists = [hog_d.compute(img).squeeze() for img in images]\n",
    "    data = np.stack(hists, 0)\n",
    "    return data\n",
    "\n",
    "# we reference the data by image to prevent making many copies\n",
    "# of the data array that DON'T have the image we're looking for\n",
    "# NOTE:  this means we must clean up our results to prevent\n",
    "#        trivial success of predicting our own (known) class\n",
    "def predict_one(data, tgts, tst_index):\n",
    "    # calculate distance from me (data[tst_index])\n",
    "    # to everyone (including myself)\n",
    "    # and pick out the top two\n",
    "    similarities = nla.norm(data[tst_index] - data, axis=1)\n",
    "    top_two = np.argpartition(similarities, 2)[:2]\n",
    "    \n",
    "    # remove me from the top two and double check i'm not there\n",
    "    prediction_idx = top_two[top_two!=tst_index][0]  # can't guess ourself\n",
    "    assert prediction_idx != tst_index               # double-double check\n",
    "\n",
    "    # return the second best overall\n",
    "    # (which is the best that is not-me)\n",
    "    return tgts[prediction_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "images = digits.images.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%timeit -r1\n",
    "# for reference, this cell takes: ~ 247 ms\n",
    "# that'll be very interesting to compare with some results from next week\n",
    "data = extract_features(images)\n",
    "\n",
    "# ugly, please don't tell anyone we did this:\n",
    "# predict_one(data, digits.target, 0)\n",
    "predicted_classes = [predict_one(data, digits.target, idx) for idx in range(len(images))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predicted_classes) == len(digits.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(digits.target, predicted_classes)\n",
    "print(\"confusion matrix:\\n\", cm)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot=True, fmt='3d')\n",
    "plt.gca().set_ylabel('Actual')\n",
    "plt.gca().set_xlabel('Predicted');\n",
    "\n",
    "# YAY!  err, um?  how did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can simplify a bit by just considering \"zero\" as our only class of interest\n",
    "# so, we have 0 and not-0\n",
    "expected_classes_z  = np.where(digits.target == 0, 0, 1)  # 1 for \"any other digit\"\n",
    "predicted_classes_z = np.where(np.array(predicted_classes) == 0, 0, 1)\n",
    "\n",
    "cm = metrics.confusion_matrix(expected_classes_z, predicted_classes_z)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='3d')\n",
    "plt.gca().set_ylabel('Actual')\n",
    "plt.gca().set_xlabel('Predicted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = cm[0,0] / cm[0,:].sum()    # correct against row (value for a real 0)\n",
    "precision = cm[0,0] / cm[:,0].sum() # correct against col (value for a pred 0)\n",
    "precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're in a better position to understand the output of the \"full\" classification report.  This is the comparison of one class taken against all the other for each of the different digits.  Overall, we didn't do too badly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(digits.target, predicted_classes))\n",
    "# note: avg here is not a simple average of the raw precisions:\n",
    "#       it takes into account the various TPs wrt to the population\n",
    "#       prevelence (all of those digits) and total predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shi Tomasi on a Checkers Board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Shi-Tomasi seems like a relatively simple, direct method, let's see if its operation matches our intuition.  What happens when we run Shi-Tomasi on a checkers board?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners = cv2.goodFeaturesToTrack(board,25,0.01,10).squeeze().astype(np.int32)\n",
    "scene = cv2.cvtColor(board, cv2.COLOR_GRAY2RGB)\n",
    "[cv2.circle(scene, (x,y), 10, [255,0,0], -1) for x,y in corners] # -1 -> filled circle\n",
    "my_show(plt.gca(), scene)\n",
    "\n",
    "# argh, why didn't that find all the corners?  can you find an argument we need to tweek?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student section here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features for Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go back to the outdoors image from week one, we can use that as another example of direct matching of features in an image.  Try it out.  That is, try to match the rider to the position in the scene.  While you're at it, try and factor the \"script-cell\" into a function that takes in an overall scene and a target/roi/box as input and returns the matched features and the match image that we displayed above.  Then, we can use that function instead of just cutting and pasting that code many time when you want to do the matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outdoors = my_read(img_dir+'data/farm-drop.jpg')\n",
    "roi = outdoors[81:800, 1001:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_helper(scene, box):\n",
    "    FLANN_INDEX_LSH = 6\n",
    "    index_params_orb = dict(algorithm = FLANN_INDEX_LSH, \n",
    "                            table_number = 6,      # 12\n",
    "                            key_size = 12,         # 20\n",
    "                            multi_probe_level = 1) # 2\n",
    "\n",
    "    # parameters for search process\n",
    "    search_params = dict(checks=50)   # or pass empty dictionary\n",
    "\n",
    "    # find the keypoints and descriptors with ORB\n",
    "    orb = cv2.ORB_create()\n",
    "    key_points_box,   descriptors_box   = orb.detectAndCompute(box, None)\n",
    "    key_points_scene, descriptors_scene = orb.detectAndCompute(scene,None)\n",
    "    print(len(key_points_box))\n",
    "\n",
    "    # repeat if we have a failure\n",
    "    matches = [[]]\n",
    "    while any(len(m) < 2 for m in matches):\n",
    "        matches = (cv2.FlannBasedMatcher(index_params_orb, search_params)\n",
    "                      .knnMatch(descriptors_box, descriptors_scene, k=2))\n",
    "    matches = [[m] for m,n in matches if m.distance < 0.75 * n.distance] \n",
    "\n",
    "    draw_params = dict(matchColor = (0,255,0),\n",
    "                       singlePointColor = (255,0,0),\n",
    "                       flags = 2)#changed flags=0 to flags=2\n",
    "    match_img = cv2.drawMatchesKnn(box, key_points_box, \n",
    "                                   scene, key_points_scene, \n",
    "                                   matches, None, **draw_params)\n",
    "    return matches, match_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT!\n",
    "outdoors = my_read_g(img_dir+'data/farm-drop.jpg')\n",
    "biker    =  outdoors[81:800, 1001:1500]\n",
    "\n",
    "matches, match_img = match_helper(outdoors, biker)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "my_show(plt.gca(), match_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we saw the following confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = digits.images.astype(np.uint8)\n",
    "\n",
    "data = extract_features(images)\n",
    "predicted_classes = [predict_one(data, digits.target, idx) for idx in range(len(images))]\n",
    "\n",
    "cm = metrics.confusion_matrix(digits.target, predicted_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='3d')\n",
    "plt.gca().set_ylabel('Actual')\n",
    "plt.gca().set_xlabel('Predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes 1 and 7 relatively different from other digits?  Why do you think they get confused for each other?  What other rows (source population) and columns (predictions) show some distinct behavior off the diagonal?  What might explain these difficulties?  Would you have noticed these relationships between reality and predictions if you had *only* used an evaluation scheme like accuracy or AUC?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features Shoot-out Showdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's propose a grand tournament of the following HOG-based feature generating methods:\n",
    "\n",
    "      * undirected, 9 bin\n",
    "      * directed, 9 bins\n",
    "      * undirected, 18 bins\n",
    "      * directed, 18 bins\n",
    "\n",
    "We'll evaluate semi-quantitatively by looking at the confusion matrices and quantitatively by looking at the accuracies.  It is reasonable to use accuracy here because the occurances of the classes are fairly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lots of placeholder arguments\n",
    "hog_base_args = ((8,8), (8,8), (8,8), (8,8))\n",
    "hog_add_args  = (1, -1.0, 0, 0.2, 1, 64)\n",
    "\n",
    "preds = {}\n",
    "for dness, bins in it.product([False, True], [9, 18]):\n",
    "    # add in experimental arguments\n",
    "    full_args = hog_base_args + (bins,) + hog_add_args + (dness,)\n",
    "    hog_d = cv2.HOGDescriptor(*full_args)\n",
    "    \n",
    "    data = extract_features(images, hog_d=hog_d)\n",
    "    predicted_classes = [predict_one(data, digits.target, idx) for idx in range(len(images))]\n",
    "    \n",
    "    preds[(dness, bins)] = predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(10,10))\n",
    "\n",
    "for ax, cnds in zip(axes.flat, preds):\n",
    "    acc = metrics.accuracy_score(digits.target, preds[cnds])\n",
    "    \n",
    "    cm = metrics.confusion_matrix(digits.target, preds[cnds])\n",
    "    sns.heatmap(cm, annot=True, fmt='3d', ax=ax)\n",
    "    \n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_xlabel('Predicted');\n",
    "    ax.set_title(\"Directed,Bins={} | Acc={:5.4f}\".format(cnds, acc))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image stitching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we'll need the `imutils` package. You can install this by using:\n",
    "\n",
    "```\n",
    "conda install -c mlgill imutils \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Stitcher:\n",
    "    def __init__(self):\n",
    "        # determine if we are using OpenCV v3.X\n",
    "        self.isv3 = imutils.is_cv3()\n",
    "\n",
    "    def stitch(self, images, ratio=0.75, reprojThresh=4.0,\n",
    "        showMatches=False):\n",
    "        # unpack the images, then detect keypoints and extract\n",
    "        # local invariant descriptors from them\n",
    "        (imageB, imageA) = images\n",
    "        (kpsA, featuresA) = self.detectAndDescribe(imageA)\n",
    "        (kpsB, featuresB) = self.detectAndDescribe(imageB)\n",
    " \n",
    "        # match features between the two images\n",
    "        M = self.matchKeypoints(kpsA, kpsB,\n",
    "            featuresA, featuresB, ratio, reprojThresh)\n",
    " \n",
    "        # if the match is None, then there aren't enough matched\n",
    "        # keypoints to create a panorama\n",
    "        if M is None:\n",
    "            return None\n",
    "\n",
    "        # otherwise, apply a perspective warp to stitch the images\n",
    "        # together\n",
    "        (matches, H, status) = M\n",
    "        result = cv2.warpPerspective(imageA, H,\n",
    "            (imageA.shape[1] + imageB.shape[1], imageA.shape[0]))\n",
    "        result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB\n",
    " \n",
    "        # check to see if the keypoint matches should be visualized\n",
    "        if showMatches:\n",
    "            vis = self.drawMatches(imageA, imageB, kpsA, kpsB, matches,\n",
    "                status)\n",
    " \n",
    "            # return a tuple of the stitched image and the\n",
    "            # visualization\n",
    "            return (result, vis)\n",
    " \n",
    "        # return the stitched image\n",
    "        return result\n",
    "    \n",
    "    def detectAndDescribe(self, image):\n",
    "        # convert the image to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "        # check to see if we are using OpenCV 3.X\n",
    "        if self.isv3:\n",
    "            # detect and extract features from the image\n",
    "            descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "            (kps, features) = descriptor.detectAndCompute(image, None)\n",
    " \n",
    "        # otherwise, we are using OpenCV 2.4.X\n",
    "        else:\n",
    "            # detect keypoints in the image\n",
    "            detector = cv2.FeatureDetector_create(\"SIFT\")\n",
    "            kps = detector.detect(gray)\n",
    " \n",
    "            # extract features from the image\n",
    "            extractor = cv2.DescriptorExtractor_create(\"SIFT\")\n",
    "            (kps, features) = extractor.compute(gray, kps)\n",
    " \n",
    "        # convert the keypoints from KeyPoint objects to NumPy\n",
    "        # arrays\n",
    "        kps = np.float32([kp.pt for kp in kps])\n",
    " \n",
    "        # return a tuple of keypoints and features\n",
    "        return (kps, features)\n",
    "    \n",
    "    def matchKeypoints(self, kpsA, kpsB, featuresA, featuresB,\n",
    "        ratio, reprojThresh):\n",
    "        # compute the raw matches and initialize the list of actual\n",
    "        # matches\n",
    "        matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "        rawMatches = matcher.knnMatch(featuresA, featuresB, 2)\n",
    "        matches = []\n",
    "\n",
    "        # loop over the raw matches\n",
    "        for m in rawMatches:\n",
    "            # ensure the distance is within a certain ratio of each\n",
    "            # other (i.e. Lowe's ratio test)\n",
    "            if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
    "                matches.append((m[0].trainIdx, m[0].queryIdx))\n",
    "\n",
    "        # computing a homography requires at least 4 matches\n",
    "        if len(matches) > 4:\n",
    "            # construct the two sets of points\n",
    "            ptsA = np.float32([kpsA[i] for (_, i) in matches])\n",
    "            ptsB = np.float32([kpsB[i] for (i, _) in matches])\n",
    "\n",
    "            # compute the homography between the two sets of points\n",
    "            (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,\n",
    "                reprojThresh)\n",
    "\n",
    "            # return the matches along with the homograpy matrix\n",
    "            # and status of each matched point\n",
    "            return (matches, H, status)\n",
    "\n",
    "        # otherwise, no homograpy could be computed\n",
    "        return None\n",
    "\n",
    "    def drawMatches(self, imageA, imageB, kpsA, kpsB, matches, status):\n",
    "        # initialize the output visualization image\n",
    "        (hA, wA) = imageA.shape[:2]\n",
    "        (hB, wB) = imageB.shape[:2]\n",
    "        vis = np.zeros((max(hA, hB), wA + wB, 3), dtype=\"uint8\")\n",
    "        vis[0:hA, 0:wA] = imageA\n",
    "        vis[0:hB, wA:] = imageB\n",
    "\n",
    "        # loop over the matches\n",
    "        for ((trainIdx, queryIdx), s) in zip(matches, status):\n",
    "            # only process the match if the keypoint was successfully\n",
    "            # matched\n",
    "            if s == 1:\n",
    "                # draw the match\n",
    "                ptA = (int(kpsA[queryIdx][0]), int(kpsA[queryIdx][1]))\n",
    "                ptB = (int(kpsB[trainIdx][0]) + wA, int(kpsB[trainIdx][1]))\n",
    "                cv2.line(vis, ptA, ptB, (0, 255, 0), 1)\n",
    "\n",
    "        # return the visualization\n",
    "        return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the two images and resize them to have a width of 400 pixels\n",
    "# (for faster processing)\n",
    "imageA = cv2.imread(\"./images/bryce_left_01.png\")\n",
    "imageB = cv2.imread(\"./images/bryce_right_01.png\")\n",
    "imageA = imutils.resize(imageA, width=400)\n",
    "imageB = imutils.resize(imageB, width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitcher = Stitcher()\n",
    "(result, vis) = stitcher.stitch([imageA, imageB], showMatches=True)\n",
    "\n",
    "# show the images\n",
    "my_show(plt.gca(), imageA, title=\"Image A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_show(plt.gca(), imageB, title=\"Image B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_show(plt.gca(), vis, title=\"Keypoint Matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_show(plt.gca(), result, title=\"Result\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 OpenCV3 (Forge)",
   "language": "python",
   "name": "opencv-forge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
