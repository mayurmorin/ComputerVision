{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# default data download is to ~/.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the big examples: (based on examples from:\n",
    "#     https://github.com/fchollet/keras/tree/master/examples)\n",
    "# mlp mnist, cnn mnist, cnn cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIXME:  add model names so we can refer back to them \n",
    "#     (i.e., they don't clobber each other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.datasets import mnist, cifar10\n",
    "\n",
    "# don't usually prefer/do direct imports, but so be it ...\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Activation, \n",
    "                          Dense, Dropout, Flatten,\n",
    "                          Conv2D, MaxPooling2D)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Friend - Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma = \\frac{1}{1 + e^{-x}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# Plot The sigmoid function\n",
    "xs = np.linspace(-10, 10, num=100, dtype=np.float32)\n",
    "activation = sigmoid(xs)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.plot(xs, activation)\n",
    "plt.plot(0,.5,'ro')\n",
    "\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='y')\n",
    "plt.axvline(x=0, color='y')\n",
    "plt.ylim([-0.1, 1.15]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Tiny Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OR Logic\n",
    "A logic gate takes in two boolean (true/false or 1/0) inputs, and returns either a 0 or 1 depending on its rule. The truth table for a logic gate shows the outputs for each combination of inputs: (0, 0), (0, 1), (1,0), and (1, 1). For example, let's look at the truth table for an Or-gate:\n",
    "\n",
    "<table>\n",
    "<tr><th colspan=\"3\">OR gate truth table</th></tr>\n",
    "<tr><th colspan=\"2\">Input</th><th>Output</th></tr>\n",
    "<tr><td>0</td><td>0</td><td>0</td></tr>\n",
    "<tr><td>0</td><td>1</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>0</td><td>1</td></tr>\n",
    "<tr><td>1</td><td>1</td><td>1</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OR as a Neuron\n",
    "\n",
    "A neuron that uses the sigmoid activation function outputs a value between (0, 1). This naturally leads us to think about boolean values. Imagine a neuron that takes in two inputs, $x_1$ and $x_2$, and a bias term:\n",
    "\n",
    "<img src=\"./logic01.png\" width=50%/>\n",
    "\n",
    "By limiting the inputs of $x_1$ and $x_2$ to be in $\\left\\{0, 1\\right\\}$, we can simulate the effect of logic gates with our neuron. The goal is to find the weights (represented by ? marks above), such that it returns an output close to 0 or 1 depending on the inputs.  What weights should we use to output the same results as OR? Remember: $\\sigma(z)$ is close to 0 when $z$ is largely negative (around -10 or less), and is close to 1 when $z$ is largely positive (around +10 or greater).\n",
    "\n",
    "$$\n",
    "z = w_1 x_1 + w_2 x_2 + b\n",
    "$$\n",
    "\n",
    "Let's think this through:\n",
    "\n",
    "* When $x_1$ and $x_2$ are both 0, the only value affecting $z$ is $b$. Because we want the result for input (0, 0) to be close to zero, $b$ should be negative (at least -10) to get the very left-hand part of the sigmoid.\n",
    "* If either $x_1$ or $x_2$ is 1, we want the output to be close to 1. That means the weights associated with $x_1$ and $x_2$ should be enough to offset $b$ to the point of causing $z$ to be at least 10 (i.e., to the far right part of the sigmoid).\n",
    "\n",
    "Let's give $b$ a value of -10. How big do we need $w_1$ and $w_2$ to be?  At least +20 will get us to +10 for just one of $\\{w_1, w_2\\}$ being on.\n",
    "\n",
    "So let's try out $w_1=20$, $w_2=20$, and $b=-10$:\n",
    "\n",
    "<img src=\"./logic02.png\" width=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you break out pencil-and-paper, we'll find this works wonderfully.  That's great.  But we don't really want to have to do a thought exercise everytime we encounter a new dataset.  How can we get an automated process to learn these weights for us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[0,0,0],\n",
    "                 [1,0,1],\n",
    "                 [0,1,1],\n",
    "                 [1,1,1]], dtype=np.float64)\n",
    "x_train, y_train = data[:,0:2], data[:,2]\n",
    "\n",
    "# build the network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(1, activation='sigmoid', input_shape=(2,)))\n",
    "\n",
    "# define \"correctness\" (the optimization problem)\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# fit the model wrt the optimization\n",
    "model.fit(x_train, y_train, epochs=5000, verbose=False)\n",
    "\n",
    "# results\n",
    "print(\"learned weights:\\n\", \n",
    "      model.layers[0].get_weights()[0], \n",
    "      model.layers[0].get_weights()[1])\n",
    "print(\"predictions:\\n\", model.predict(x_train).round().astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras MLP on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trains a simple MLP with dropout on the MNIST dataset.\n",
    "Gets to 98.40% test accuracy after 20 epochs\n",
    "(there is *a lot* of margin for parameter tuning).\n",
    "2 seconds per epoch on a K520 GPU.\n",
    "'''\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "n_train, n_test = x_train.shape[0], x_test.shape[0]\n",
    "num_classes = len(set(y_train)) # aka, 10\n",
    "\n",
    "# here we flatten the data \n",
    "# (so r,c information is lost - rely on absolute position only)\n",
    "x_train = x_train.reshape(n_train, -1).astype(np.float32) / 255.0\n",
    "x_test  = x_test.reshape( n_test, -1).astype(np.float32) / 255.0\n",
    "\n",
    "flat_shape = x_train[0].shape\n",
    "\n",
    "print('{} train and {} test examples'.format(n_train, n_test))\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test,  num_classes)\n",
    " \n",
    "# network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=flat_shape))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# display architecture\n",
    "model.summary()\n",
    "\n",
    "# define learning procedure\n",
    "opt = keras.optimizers.RMSprop()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "batch_size, epochs = 128, 2\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "# evaluate\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras CNN MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "n_train, n_test = x_train.shape[0], x_test.shape[0]\n",
    "img_shape = x_train[0].shape[1:]\n",
    "num_classes = len(set(y_train)) # aka, 10\n",
    "\n",
    "# we maintain grid structure of data\n",
    "# (and we add a dimension to account for the single channel images)\n",
    "x_train = np.expand_dims(x_train, -1).astype(np.float32) / 255.0\n",
    "x_test  = np.expand_dims(x_test,  -1).astype(np.float32) / 255.0\n",
    "input_shape = x_train[0].shape\n",
    "\n",
    "print('{} train and {} test examples'.format(n_train, n_test))\n",
    "print('image shape:', input_shape)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test,  num_classes)\n",
    " \n",
    "\n",
    "# define architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# define learning procedure\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "batch_size, epochs = 128, 2 # 128, 12\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "# evaluate\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for lenet in keras, see here\n",
    "# (and compare with the tensorflow exmaple in week 6)\n",
    "# https://www.kaggle.com/ftence/keras-cnn-inspired-by-lenet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "\n",
    "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "n_train, n_test = x_train.shape[0], x_test.shape[0]\n",
    "img_shape = x_train[0].shape\n",
    "num_classes = len(set(y_train[:,0])) # aka, 10\n",
    "\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test  = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "print('{} train and {} test examples'.format(n_train, n_test))\n",
    "print('image shape:', img_shape)\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test,  num_classes)\n",
    "\n",
    "# definte architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=img_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# define learning procedure (RMSprop optimizer)\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "batch_size, epochs = 32, 2 # 32, 50; 32,100\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "          validation_data=(x_test, y_test), shuffle=True)\n",
    "\n",
    "# evaluate trained model\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "# saved trained model\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minimal Networks for AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train a (trivial) network for AND\n",
    "# how do weights differ if you use ReLU versus sigmoid activations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[0,0,0],\n",
    "                 [1,0,0],\n",
    "                 [0,1,0],\n",
    "                 [1,1,1]], dtype=np.float64)\n",
    "x_train, y_train = data[:,0:2], data[:,2]\n",
    "\n",
    "# student section begins here\n",
    "# build the network architecture\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# student section ends here\n",
    "# results\n",
    "print(\"learned weights:\\n\", \n",
    "      model.layers[0].get_weights()[0], \n",
    "      model.layers[0].get_weights()[1])\n",
    "print(\"predictions:\\n\", model.predict(x_train).round().astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[0,0,0],\n",
    "                 [1,0,0],\n",
    "                 [0,1,0],\n",
    "                 [1,1,1]], dtype=np.float64)\n",
    "x_train, y_train = data[:,0:2], data[:,2]\n",
    "\n",
    "# student section begins here\n",
    "# build the network architecture\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# student section ends here\n",
    "\n",
    "# results\n",
    "print(\"learned weights:\\n\", \n",
    "      model.layers[0].get_weights()[0], \n",
    "      model.layers[0].get_weights()[1])\n",
    "print(\"predictions:\\n\", model.predict(x_train).round().astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP on XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mathematically, a single node can't learn XOR.  \n",
    "# we need at least a network with a hidden layer.  try it out.\n",
    "# train a MLP for XOR\n",
    "# learning rate of .01 may be too small; try .1\n",
    "# may need lots of epochs ... and due to random initialization, may have bad luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[0,0,0],\n",
    "                 [1,0,1],\n",
    "                 [0,1,1],\n",
    "                 [1,1,0]], dtype=np.float64)\n",
    "x_train, y_train = data[:,0:2], data[:,2]\n",
    "\n",
    "# network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(2, activation='sigmoid', input_shape=(2,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# opt & fit (customize learning rate)\n",
    "# this is somewhat brittle:\n",
    "my_sgd = keras.optimizers.SGD(lr=0.5)\n",
    "# this is more robust, but we haven't taken a deep dive into optimization parameters\n",
    "# my_sgd = keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_squared_error', optimizer=my_sgd, metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10000, verbose=False)\n",
    "\n",
    "# results\n",
    "print(\"learned weights:\", \n",
    "      model.layers[0].get_weights()[0], \n",
    "      model.layers[0].get_weights()[1],\n",
    "      model.layers[1].get_weights()[0], \n",
    "      model.layers[1].get_weights()[1], sep=\"\\n\")\n",
    "print(\"predictions:\\n\", \n",
    "      model.predict(x_train).round().astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### parameters of cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many parameters were in the cnn cifar model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mlp cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mlp network on cifar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "n_train, n_test = x_train.shape[0], x_test.shape[0]\n",
    "img_shape = x_train[0].shape\n",
    "num_classes = len(set(y_train[:,0])) # aka, 10\n",
    "\n",
    "x_train = x_train.reshape(n_train, -1).astype(np.float32) / 255.0\n",
    "x_test  = x_test.reshape( n_test, -1).astype(np.float32) / 255.0\n",
    "flat_shape = x_train[0].shape\n",
    "\n",
    "print('{} train and {} test examples'.format(n_train, n_test))\n",
    "print('image shape:', img_shape)\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test,  num_classes)\n",
    "\n",
    "\n",
    "# network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=flat_shape))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# display architecture\n",
    "model.summary()\n",
    "\n",
    "# define learning procedure\n",
    "opt = keras.optimizers.RMSprop()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "batch_size, epochs = 128, 2\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "# evaluate\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cnn 101categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cnn cifar on 101cat (i.e., train(cifar), test(101cat) \"transfer learning\")\n",
    "# do this via two steps:\n",
    "# before doing any network processing, use cv2 to resize all images to a \"standard\" size\n",
    "# (annoying, but keras doesn't have a nice resize layer - tensorflow does)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 OpenCV3 (Forge)",
   "language": "python",
   "name": "opencv-forge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
